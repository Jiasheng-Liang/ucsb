import requests
import time
from bs4 import BeautifulSoup
from datetime import datetime
import json
import os
import hashlib
import pandas as pd


url = 'https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions'
base_url = 'https://www.presidency.ucsb.edu'
save_directory = '/home/jiasheng/fstore/data_ucsb/Jsonfiles'
file_count=1

def make_absolute(url):
    if url.startswith('/'):
        return base_url + url
    return url

def get_all_links(page):
    url = "https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions?page=" + str(page)
    response = requests.get(url)
    time.sleep(1)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find all divs with class 'field-title'
    title_divs = soup.find_all('div', class_='field-title')
    
    links = []
    for div in title_divs:
        # Find all a elements (links) within each div
        a_elements = div.find_all('a')
        for a in a_elements:
            # Get the href attribute of the a element (the link) and add it to the list
            link_text = a.text
            title = link_text.strip()
            links.append((a['href'], title))

    return links



def scrape_website(url, title):
    global file_count
    response = requests.get(url)
    time.sleep(1)
    soup = BeautifulSoup(response.text, 'html.parser')

    date_element_div = soup.find('div', class_='field-docs-start-date-time')
    date_element = date_element_div.find('span', class_='date-display-single') if date_element_div else None
    date = date_element.text.strip() if date_element else 'N/A'

    # convert date string to datetime object
    date_object = datetime.strptime(date, "%B %d, %Y") if date != 'N/A' else None

    # if date is before 2014 or couldn't find date, return False
    if date_object is None or date_object.year < 2014:
        return False, ''

    # Get location from the 'div' with class 'field-spot-state'
    location_div = soup.find('div', class_='field-spot-state')
    location = location_div.text.strip() if location_div else 'N/A'

    #get speaker
    speaker_element_div = soup.find('div', class_='field-docs-person')
    speaker_element = speaker_element_div.find('div') if speaker_element_div else None
    speaker = speaker_element['about'].split('/')[-1] if speaker_element else 'N/A'

    # get document text
    document_text_div = soup.find('div', class_='field-docs-content')
    document_text = document_text_div.text if document_text_div else 'N/A'

    data = {
        'date': date,
        'title': title,
        'location': location,
        'speaker': speaker,
        'document_text': document_text,
        'raw_html': str(soup),
        'source_url': url,
        'accessed_at': str(datetime.utcnow())
    }

    # Generate a hash value from the URL
    hash_value = hashlib.md5(url.encode()).hexdigest()
    
    # Use the hash value as the filename
    filename = f"{hash_value}.json"
    filepath = os.path.join(save_directory, filename)


    with open(filepath, 'w') as f:
        json.dump(data, f)

    return data, filename


def main():
    page = 0
    all_data = pd.DataFrame(columns=['hash_value', 'title', 'url', 'date', 'speaker'])
    
    while True:  # keep going to the next page
        print(f"Processing page {page+1}")
        links = get_all_links(page)
        
        if not links:  # no more pages, stop the loop
            break
            
        for link, title in links:
            link = make_absolute(link)
            data, filename = scrape_website(link, title)
            
            if data is not False:
                print(f"Saved file: {filename}")
                
                data_row = {
                    'hash_value': filename.replace('.json', ''),
                    'title': title,
                    'url': link,
                    'date': data['date'],
                    'speaker': data['speaker']
                }
                all_data = pd.concat([all_data, pd.DataFrame([data_row])], ignore_index=True)
            else:
                print(f"Skipped link: {link}")
                # Save the CSV before ending the loop
                all_data.to_csv('alldata.csv', index=False)
                return  # stop the loop if the date is before 2014 or couldn't find the date
                
        page += 1  # go to the next page

main()
